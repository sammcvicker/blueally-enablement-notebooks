{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "92495f50-72d6-4181-863a-64578fb2cc61",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kamiwaza_client import KamiwazaClient\n",
    "import openai\n",
    "from pydantic import BaseModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "45925bd1-e294-4e6f-a6bd-a8446e6ba5f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ActiveModelDeployment(id=UUID('9974c1d7-444d-4f78-9bf6-30ecef701d43'), m_id=UUID('c45afd07-17b5-4509-b026-33c43e3b5467'), m_name='Qwen3-32B-AWQ', status='DEPLOYED', instances=[ModelInstance:\n",
       " ID: 70b4c6ea-41fa-458a-8e0a-2555c262a654\n",
       " Deployment ID: 9974c1d7-444d-4f78-9bf6-30ecef701d43\n",
       " Status: DEPLOYED\n",
       " Listen Port: 32775], lb_port=51105, endpoint='http://34.230.49.204:51105/v1')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client = KamiwazaClient(\"http://34.230.49.204:7777/api/\")\n",
    "client.serving.list_active_deployments()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0e29f9b9-b838-476b-b974-6ff466fc6a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_client = client.openai.get_client('Qwen3-32B-AWQ')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88e1551c-8a67-48cc-9f87-671ed5c1998c",
   "metadata": {},
   "source": [
    "## Define the runtime tool & its JSON schema\n",
    "We expose a dummy weather function so the demo is entirely self‑contained.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "62c8853b-8024-475f-a696-1b9b0072195c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Tool registered — ready for the chat call\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/mc/ls40y_m57zz0hw7gjzblhxpc0000gn/T/ipykernel_49232/2537202073.py:28: PydanticDeprecatedSince20: The `schema` method is deprecated; use `model_json_schema` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/\n",
      "  \"parameters\": WeatherParams.schema(),  # Pydantic → JSON Schema\n"
     ]
    }
   ],
   "source": [
    "from typing import Dict, Any\n",
    "import json, time\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "class WeatherParams(BaseModel):\n",
    "    city: str = Field(..., description=\"City name\")\n",
    "    state: str = Field(..., description=\"US state abbreviation, e.g. CA\")\n",
    "    unit: str = Field(..., description=\"Either 'celsius' or 'fahrenheit'\")\n",
    "\n",
    "def get_current_weather(city: str, state: str, unit: str) -> Dict[str, Any]:\n",
    "    \"\"\"Pretend weather service so we don't need a real API call.\"\"\"\n",
    "    fake_temp_f = 100  # you could randomise this or call a real API\n",
    "    temp = fake_temp_f if unit == \"fahrenheit\" else round((fake_temp_f - 32) * 5 / 9)\n",
    "    time.sleep(0.3)  # small pause so the streaming looks lively\n",
    "    return {\n",
    "        \"temperature\": temp,\n",
    "        \"unit\": unit,\n",
    "        \"city\": city,\n",
    "        \"state\": state,\n",
    "        \"description\": \"clear skies with a light breeze\",\n",
    "    }\n",
    "\n",
    "weather_tool_schema = {\n",
    "    \"type\": \"function\",\n",
    "    \"function\": {\n",
    "        \"name\": \"get_current_weather\",\n",
    "        \"description\": \"Get the current weather in a given US city\",\n",
    "        \"parameters\": WeatherParams.schema(),  # Pydantic → JSON Schema\n",
    "    },\n",
    "}\n",
    "\n",
    "tools = [weather_tool_schema]\n",
    "\n",
    "local_tool_registry = {\"get_current_weather\": get_current_weather}\n",
    "print(\"✅ Tool registered — ready for the chat call\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8821199d-66e1-4d37-add3-4e5f44ae810b",
   "metadata": {},
   "source": [
    "## Kick off a streaming chat with tool support\n",
    "The system prompt asks the model to *think in public* using `<thinking>` \n",
    "tags, then decide whether to call the weather tool.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5a6377a2-d5e8-4ed0-92f4-a694266e5ed1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-11 12:15:05,181 - httpx - INFO - HTTP Request: POST http://34.230.49.204:51105/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📡 **Streaming** — watch the tokens, tool call, and final answer arrive\n",
      "\n",
      "\u001b[36m\n",
      "\u001b[0m\u001b[36mOkay\u001b[0m\u001b[36m,\u001b[0m\u001b[36m the\u001b[0m\u001b[36m user\u001b[0m\u001b[36m is\u001b[0m\u001b[36m asking\u001b[0m\u001b[36m whether\u001b[0m\u001b[36m they\u001b[0m\u001b[36m should\u001b[0m\u001b[36m wear\u001b[0m\u001b[36m short\u001b[0m\u001b[36m sleeves\u001b[0m\u001b[36m or\u001b[0m\u001b[36m long\u001b[0m\u001b[36m sleeves\u001b[0m\u001b[36m in\u001b[0m\u001b[36m NYC\u001b[0m\u001b[36m today\u001b[0m\u001b[36m.\u001b[0m\u001b[36m To\u001b[0m\u001b[36m figure\u001b[0m\u001b[36m this\u001b[0m\u001b[36m out\u001b[0m\u001b[36m,\u001b[0m\u001b[36m I\u001b[0m\u001b[36m need\u001b[0m\u001b[36m to\u001b[0m\u001b[36m know\u001b[0m\u001b[36m the\u001b[0m\u001b[36m current\u001b[0m\u001b[36m weather\u001b[0m\u001b[36m there\u001b[0m\u001b[36m.\u001b[0m\u001b[36m The\u001b[0m\u001b[36m tools\u001b[0m\u001b[36m provided\u001b[0m\u001b[36m include\u001b[0m\u001b[36m a\u001b[0m\u001b[36m function\u001b[0m\u001b[36m called\u001b[0m\u001b[36m get\u001b[0m\u001b[36m_current\u001b[0m\u001b[36m_weather\u001b[0m\u001b[36m,\u001b[0m\u001b[36m which\u001b[0m\u001b[36m requires\u001b[0m\u001b[36m city\u001b[0m\u001b[36m,\u001b[0m\u001b[36m state\u001b[0m\u001b[36m,\u001b[0m\u001b[36m and\u001b[0m\u001b[36m unit\u001b[0m\u001b[36m parameters\u001b[0m\u001b[36m.\u001b[0m\u001b[36m \n",
      "\n",
      "\u001b[0m\u001b[36mFirst\u001b[0m\u001b[36m,\u001b[0m\u001b[36m NYC\u001b[0m\u001b[36m is\u001b[0m\u001b[36m New\u001b[0m\u001b[36m York\u001b[0m\u001b[36m City\u001b[0m\u001b[36m,\u001b[0m\u001b[36m so\u001b[0m\u001b[36m the\u001b[0m\u001b[36m city\u001b[0m\u001b[36m parameter\u001b[0m\u001b[36m would\u001b[0m\u001b[36m be\u001b[0m\u001b[36m \"\u001b[0m\u001b[36mNew\u001b[0m\u001b[36m York\u001b[0m\u001b[36m\"\u001b[0m\u001b[36m and\u001b[0m\u001b[36m the\u001b[0m\u001b[36m state\u001b[0m\u001b[36m is\u001b[0m\u001b[36m \"\u001b[0m\u001b[36mNY\u001b[0m\u001b[36m\".\u001b[0m\u001b[36m The\u001b[0m\u001b[36m unit\u001b[0m\u001b[36m could\u001b[0m\u001b[36m be\u001b[0m\u001b[36m either\u001b[0m\u001b[36m c\u001b[0m\u001b[36melsius\u001b[0m\u001b[36m or\u001b[0m\u001b[36m f\u001b[0m\u001b[36mahrenheit\u001b[0m\u001b[36m.\u001b[0m\u001b[36m Since\u001b[0m\u001b[36m the\u001b[0m\u001b[36m user\u001b[0m\u001b[36m is\u001b[0m\u001b[36m likely\u001b[0m\u001b[36m in\u001b[0m\u001b[36m the\u001b[0m\u001b[36m US\u001b[0m\u001b[36m,\u001b[0m\u001b[36m they\u001b[0m\u001b[36m might\u001b[0m\u001b[36m prefer\u001b[0m\u001b[36m f\u001b[0m\u001b[36mahrenheit\u001b[0m\u001b[36m,\u001b[0m\u001b[36m but\u001b[0m\u001b[36m I\u001b[0m\u001b[36m should\u001b[0m\u001b[36m check\u001b[0m\u001b[36m if\u001b[0m\u001b[36m there\u001b[0m\u001b[36m's\u001b[0m\u001b[36m a\u001b[0m\u001b[36m default\u001b[0m\u001b[36m.\u001b[0m\u001b[36m The\u001b[0m\u001b[36m function\u001b[0m\u001b[36m requires\u001b[0m\u001b[36m all\u001b[0m\u001b[36m three\u001b[0m\u001b[36m parameters\u001b[0m\u001b[36m,\u001b[0m\u001b[36m so\u001b[0m\u001b[36m I\u001b[0m\u001b[36m'll\u001b[0m\u001b[36m have\u001b[0m\u001b[36m to\u001b[0m\u001b[36m specify\u001b[0m\u001b[36m them\u001b[0m\u001b[36m.\n",
      "\n",
      "\u001b[0m\u001b[36mI\u001b[0m\u001b[36m need\u001b[0m\u001b[36m to\u001b[0m\u001b[36m call\u001b[0m\u001b[36m the\u001b[0m\u001b[36m get\u001b[0m\u001b[36m_current\u001b[0m\u001b[36m_weather\u001b[0m\u001b[36m function\u001b[0m\u001b[36m with\u001b[0m\u001b[36m these\u001b[0m\u001b[36m details\u001b[0m\u001b[36m.\u001b[0m\u001b[36m Let\u001b[0m\u001b[36m me\u001b[0m\u001b[36m make\u001b[0m\u001b[36m sure\u001b[0m\u001b[36m the\u001b[0m\u001b[36m parameters\u001b[0m\u001b[36m are\u001b[0m\u001b[36m correct\u001b[0m\u001b[36m.\u001b[0m\u001b[36m City\u001b[0m\u001b[36m:\u001b[0m\u001b[36m New\u001b[0m\u001b[36m York\u001b[0m\u001b[36m,\u001b[0m\u001b[36m State\u001b[0m\u001b[36m:\u001b[0m\u001b[36m NY\u001b[0m\u001b[36m,\u001b[0m\u001b[36m Unit\u001b[0m\u001b[36m:\u001b[0m\u001b[36m f\u001b[0m\u001b[36mahrenheit\u001b[0m\u001b[36m.\u001b[0m\u001b[36m Once\u001b[0m\u001b[36m I\u001b[0m\u001b[36m get\u001b[0m\u001b[36m the\u001b[0m\u001b[36m temperature\u001b[0m\u001b[36m,\u001b[0m\u001b[36m I\u001b[0m\u001b[36m can\u001b[0m\u001b[36m advise\u001b[0m\u001b[36m on\u001b[0m\u001b[36m the\u001b[0m\u001b[36m clothing\u001b[0m\u001b[36m.\u001b[0m\u001b[36m If\u001b[0m\u001b[36m it\u001b[0m\u001b[36m's\u001b[0m\u001b[36m warm\u001b[0m\u001b[36m,\u001b[0m\u001b[36m short\u001b[0m\u001b[36m sleeves\u001b[0m\u001b[36m;\u001b[0m\u001b[36m if\u001b[0m\u001b[36m cool\u001b[0m\u001b[36m,\u001b[0m\u001b[36m long\u001b[0m\u001b[36m sleeves\u001b[0m\u001b[36m.\u001b[0m\u001b[36m Let\u001b[0m\u001b[36m me\u001b[0m\u001b[36m generate\u001b[0m\u001b[36m the\u001b[0m\u001b[36m tool\u001b[0m\u001b[36m call\u001b[0m\u001b[36m.\n",
      "\u001b[0m\n",
      "\n",
      "\n",
      "\n",
      "🔧 Model requested tool: get_current_weather\n",
      "   with args: {'city': 'New York', 'state': 'NY', 'unit': 'fahrenheit'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-11 12:15:13,823 - httpx - INFO - HTTP Request: POST http://34.230.49.204:51105/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔧 Tool response: {'temperature': 100, 'unit': 'fahrenheit', 'city': 'New York', 'state': 'NY', 'description': 'clear skies with a light breeze'}\n",
      "\n",
      "\n",
      "💭 Getting final answer...\n",
      "\n",
      "\u001b[36m\n",
      "\u001b[0m\u001b[36mOkay\u001b[0m\u001b[36m,\u001b[0m\u001b[36m the\u001b[0m\u001b[36m user\u001b[0m\u001b[36m is\u001b[0m\u001b[36m asking\u001b[0m\u001b[36m whether\u001b[0m\u001b[36m to\u001b[0m\u001b[36m wear\u001b[0m\u001b[36m short\u001b[0m\u001b[36m sleeves\u001b[0m\u001b[36m or\u001b[0m\u001b[36m long\u001b[0m\u001b[36m sleeves\u001b[0m\u001b[36m in\u001b[0m\u001b[36m NYC\u001b[0m\u001b[36m today\u001b[0m\u001b[36m.\u001b[0m\u001b[36m I\u001b[0m\u001b[36m need\u001b[0m\u001b[36m to\u001b[0m\u001b[36m check\u001b[0m\u001b[36m the\u001b[0m\u001b[36m weather\u001b[0m\u001b[36m.\u001b[0m\u001b[36m I\u001b[0m\u001b[36m'll\u001b[0m\u001b[36m use\u001b[0m\u001b[36m the\u001b[0m\u001b[36m get\u001b[0m\u001b[36m_current\u001b[0m\u001b[36m_weather\u001b[0m\u001b[36m tool\u001b[0m\u001b[36m for\u001b[0m\u001b[36m New\u001b[0m\u001b[36m York\u001b[0m\u001b[36m.\u001b[0m\u001b[36m The\u001b[0m\u001b[36m response\u001b[0m\u001b[36m says\u001b[0m\u001b[36m \u001b[0m\u001b[36m1\u001b[0m\u001b[36m0\u001b[0m\u001b[36m0\u001b[0m\u001b[36m°F\u001b[0m\u001b[36m with\u001b[0m\u001b[36m clear\u001b[0m\u001b[36m skies\u001b[0m\u001b[36m and\u001b[0m\u001b[36m a\u001b[0m\u001b[36m light\u001b[0m\u001b[36m breeze\u001b[0m\u001b[36m.\u001b[0m\u001b[36m That\u001b[0m\u001b[36m's\u001b[0m\u001b[36m pretty\u001b[0m\u001b[36m hot\u001b[0m\u001b[36m.\u001b[0m\u001b[36m At\u001b[0m\u001b[36m \u001b[0m\u001b[36m1\u001b[0m\u001b[36m0\u001b[0m\u001b[36m0\u001b[0m\u001b[36m°F\u001b[0m\u001b[36m,\u001b[0m\u001b[36m short\u001b[0m\u001b[36m sleeves\u001b[0m\u001b[36m would\u001b[0m\u001b[36m be\u001b[0m\u001b[36m more\u001b[0m\u001b[36m comfortable\u001b[0m\u001b[36m.\u001b[0m\u001b[36m Long\u001b[0m\u001b[36m sleeves\u001b[0m\u001b[36m might\u001b[0m\u001b[36m make\u001b[0m\u001b[36m them\u001b[0m\u001b[36m over\u001b[0m\u001b[36mheat\u001b[0m\u001b[36m.\u001b[0m\u001b[36m But\u001b[0m\u001b[36m maybe\u001b[0m\u001b[36m suggest\u001b[0m\u001b[36m light\u001b[0m\u001b[36m clothing\u001b[0m\u001b[36m and\u001b[0m\u001b[36m mention\u001b[0m\u001b[36m the\u001b[0m\u001b[36m breeze\u001b[0m\u001b[36m.\u001b[0m\u001b[36m Also\u001b[0m\u001b[36m,\u001b[0m\u001b[36m remind\u001b[0m\u001b[36m about\u001b[0m\u001b[36m sun\u001b[0m\u001b[36m protection\u001b[0m\u001b[36m since\u001b[0m\u001b[36m it\u001b[0m\u001b[36m's\u001b[0m\u001b[36m clear\u001b[0m\u001b[36m.\u001b[0m\u001b[36m The\u001b[0m\u001b[36m answer\u001b[0m\u001b[36m should\u001b[0m\u001b[36m be\u001b[0m\u001b[36m friendly\u001b[0m\u001b[36m and\u001b[0m\u001b[36m practical\u001b[0m\u001b[36m.\n",
      "\u001b[0m\n",
      "\n",
      "Given that it's 100°F in New York City with clear skies and a light breeze, **short sleeves would be the best choice** to stay cool. Pair it with lightweight, breathable fabric and consider sunscreen or a hat for sun protection. Long sleeves might feel too warm in this heat. Enjoy the sunny day! ☀️\n",
      "\n",
      "🎉 **Done!**\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": (\n",
    "            \"You are a helpful assistant. When you receive tool outputs, \"\n",
    "            \"always provide a complete and helpful final answer to the user's question. \"\n",
    "            \"Be conversational and provide practical advice.\"\n",
    "        ),\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"should i wear short sleeves or long sleeves in nyc today?\",\n",
    "    },\n",
    "]\n",
    "\n",
    "print(\"\\n📡 **Streaming** — watch the tokens, tool call, and final answer arrive\\n\")\n",
    "\n",
    "# First API call with tools\n",
    "response_stream = openai_client.chat.completions.create(\n",
    "    model=\"model\", \n",
    "    messages=messages,\n",
    "    tools=tools,\n",
    "    tool_choice=\"auto\",  # Let the model decide when to use tools\n",
    "    stream=True,\n",
    ")\n",
    "\n",
    "# Track tool calls as they stream in\n",
    "tool_calls = []\n",
    "current_tool_call = None\n",
    "accumulated_content = \"\"\n",
    "\n",
    "for chunk in response_stream:\n",
    "    if not chunk.choices:\n",
    "        continue\n",
    "    \n",
    "    choice = chunk.choices[0]\n",
    "    delta = choice.delta\n",
    "    \n",
    "    # Handle reasoning content (if your model supports it)\n",
    "    if hasattr(delta, \"reasoning_content\") and delta.reasoning_content:\n",
    "        print(\"\\033[36m\" + delta.reasoning_content + \"\\033[0m\", end=\"\", flush=True)\n",
    "    \n",
    "    # Handle regular content\n",
    "    if delta.content:\n",
    "        accumulated_content += delta.content\n",
    "        print(delta.content, end=\"\", flush=True)\n",
    "    \n",
    "    # Handle tool calls\n",
    "    if delta.tool_calls:\n",
    "        for tc_delta in delta.tool_calls:\n",
    "            # Start a new tool call\n",
    "            if tc_delta.index == 0 and not current_tool_call:\n",
    "                current_tool_call = {\n",
    "                    \"id\": tc_delta.id or f\"call_{uuid.uuid4().hex[:8]}\",\n",
    "                    \"type\": \"function\",\n",
    "                    \"function\": {\n",
    "                        \"name\": tc_delta.function.name,\n",
    "                        \"arguments\": \"\"\n",
    "                    }\n",
    "                }\n",
    "            \n",
    "            # Accumulate arguments\n",
    "            if tc_delta.function.arguments:\n",
    "                current_tool_call[\"function\"][\"arguments\"] += tc_delta.function.arguments\n",
    "\n",
    "# Execute tool calls if any\n",
    "if choice.finish_reason == \"tool_calls\" and current_tool_call:\n",
    "    print(f\"\\n\\n🔧 Model requested tool: {current_tool_call['function']['name']}\")\n",
    "    \n",
    "    # Parse arguments and execute\n",
    "    args = json.loads(current_tool_call[\"function\"][\"arguments\"])\n",
    "    print(f\"   with args: {args}\")\n",
    "    \n",
    "    tool_name = current_tool_call[\"function\"][\"name\"]\n",
    "    tool_result = local_tool_registry[tool_name](**args)\n",
    "    print(f\"🔧 Tool response: {tool_result}\")\n",
    "    \n",
    "    # Add the assistant's tool call message\n",
    "    messages.append({\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": accumulated_content if accumulated_content else None,\n",
    "        \"tool_calls\": [current_tool_call]\n",
    "    })\n",
    "    \n",
    "    # Add the tool response\n",
    "    messages.append({\n",
    "        \"role\": \"tool\",\n",
    "        \"tool_call_id\": current_tool_call[\"id\"],\n",
    "        \"name\": tool_name,\n",
    "        \"content\": json.dumps(tool_result)\n",
    "    })\n",
    "    \n",
    "    # Make a follow-up call to get the final answer\n",
    "    print(\"\\n\\n💭 Getting final answer...\\n\")\n",
    "    \n",
    "    follow_stream = openai_client.chat.completions.create(\n",
    "        model=\"model\",\n",
    "        messages=messages,\n",
    "        stream=True,\n",
    "        # No tools this time - we want a final answer\n",
    "    )\n",
    "    \n",
    "    # Stream the final response\n",
    "    for follow_chunk in follow_stream:\n",
    "        if not follow_chunk.choices:\n",
    "            continue\n",
    "        \n",
    "        follow_choice = follow_chunk.choices[0]\n",
    "        follow_delta = follow_choice.delta\n",
    "        \n",
    "        # Handle reasoning content\n",
    "        if hasattr(follow_delta, \"reasoning_content\") and follow_delta.reasoning_content:\n",
    "            print(\"\\033[36m\" + follow_delta.reasoning_content + \"\\033[0m\", end=\"\", flush=True)\n",
    "        \n",
    "        # Handle final answer content\n",
    "        if follow_delta.content:\n",
    "            print(follow_delta.content, end=\"\", flush=True)\n",
    "    \n",
    "    print(\"\\n\\n🎉 **Done!**\")\n",
    "\n",
    "# If no tool was called, we're done\n",
    "elif choice.finish_reason == \"stop\":\n",
    "    print(\"\\n\\n🎉 **Done!** (No tool calls needed)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f18a82-683b-4860-8b3e-a7e6b27988b8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
