{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "92495f50-72d6-4181-863a-64578fb2cc61",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kamiwaza_client import KamiwazaClient\n",
    "import openai\n",
    "from pydantic import BaseModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "45925bd1-e294-4e6f-a6bd-a8446e6ba5f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ActiveModelDeployment(id=UUID('9974c1d7-444d-4f78-9bf6-30ecef701d43'), m_id=UUID('c45afd07-17b5-4509-b026-33c43e3b5467'), m_name='Qwen3-32B-AWQ', status='DEPLOYED', instances=[ModelInstance:\n",
       " ID: 70b4c6ea-41fa-458a-8e0a-2555c262a654\n",
       " Deployment ID: 9974c1d7-444d-4f78-9bf6-30ecef701d43\n",
       " Status: DEPLOYED\n",
       " Listen Port: 32775], lb_port=51105, endpoint='http://localhost:51105/v1')]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client = KamiwazaClient(\"http://localhost:7777/api/\")\n",
    "client.serving.list_active_deployments()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0e29f9b9-b838-476b-b974-6ff466fc6a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_client = client.openai.get_client('Qwen3-32B-AWQ')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88e1551c-8a67-48cc-9f87-671ed5c1998c",
   "metadata": {},
   "source": [
    "## Define the runtime tool & its JSON schema\n",
    "We expose a dummy weather function so the demo is entirely selfâ€‘contained.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "62c8853b-8024-475f-a696-1b9b0072195c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Tool registered â€” ready for the chat call\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3876565/2537202073.py:28: PydanticDeprecatedSince20: The `schema` method is deprecated; use `model_json_schema` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/\n",
      "  \"parameters\": WeatherParams.schema(),  # Pydantic â†’ JSON Schema\n"
     ]
    }
   ],
   "source": [
    "from typing import Dict, Any\n",
    "import json, time\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "class WeatherParams(BaseModel):\n",
    "    city: str = Field(..., description=\"City name\")\n",
    "    state: str = Field(..., description=\"US state abbreviation, e.g. CA\")\n",
    "    unit: str = Field(..., description=\"Either 'celsius' or 'fahrenheit'\")\n",
    "\n",
    "def get_current_weather(city: str, state: str, unit: str) -> Dict[str, Any]:\n",
    "    \"\"\"Pretend weather service so we don't need a real API call.\"\"\"\n",
    "    fake_temp_f = 100  # you could randomise this or call a real API\n",
    "    temp = fake_temp_f if unit == \"fahrenheit\" else round((fake_temp_f - 32) * 5 / 9)\n",
    "    time.sleep(0.3)  # small pause so the streaming looks lively\n",
    "    return {\n",
    "        \"temperature\": temp,\n",
    "        \"unit\": unit,\n",
    "        \"city\": city,\n",
    "        \"state\": state,\n",
    "        \"description\": \"clear skies with a light breeze\",\n",
    "    }\n",
    "\n",
    "weather_tool_schema = {\n",
    "    \"type\": \"function\",\n",
    "    \"function\": {\n",
    "        \"name\": \"get_current_weather\",\n",
    "        \"description\": \"Get the current weather in a given US city\",\n",
    "        \"parameters\": WeatherParams.schema(),  # Pydantic â†’ JSON Schema\n",
    "    },\n",
    "}\n",
    "\n",
    "tools = [weather_tool_schema]\n",
    "\n",
    "local_tool_registry = {\"get_current_weather\": get_current_weather}\n",
    "print(\"âœ… Tool registered â€” ready for the chat call\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8821199d-66e1-4d37-add3-4e5f44ae810b",
   "metadata": {},
   "source": [
    "##Â Kick off a streaming chat with tool support\n",
    "The system prompt asks the model to *think in public* using `<thinking>` \n",
    "tags, then decide whether to call the weather tool.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5a6377a2-d5e8-4ed0-92f4-a694266e5ed1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-11 12:14:57,487 - httpx - INFO - HTTP Request: POST http://localhost:51105/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“¡ **Streaming** â€” watch the tokens, tool call, and final answer arrive\n",
      "\n",
      "\u001b[36m\n",
      "\u001b[0m\u001b[36mOkay\u001b[0m\u001b[36m,\u001b[0m\u001b[36m the\u001b[0m\u001b[36m user\u001b[0m\u001b[36m is\u001b[0m\u001b[36m asking\u001b[0m\u001b[36m whether\u001b[0m\u001b[36m they\u001b[0m\u001b[36m should\u001b[0m\u001b[36m wear\u001b[0m\u001b[36m short\u001b[0m\u001b[36m sleeves\u001b[0m\u001b[36m or\u001b[0m\u001b[36m long\u001b[0m\u001b[36m sleeves\u001b[0m\u001b[36m in\u001b[0m\u001b[36m NYC\u001b[0m\u001b[36m today\u001b[0m\u001b[36m.\u001b[0m\u001b[36m To\u001b[0m\u001b[36m figure\u001b[0m\u001b[36m this\u001b[0m\u001b[36m out\u001b[0m\u001b[36m,\u001b[0m\u001b[36m I\u001b[0m\u001b[36m need\u001b[0m\u001b[36m to\u001b[0m\u001b[36m know\u001b[0m\u001b[36m the\u001b[0m\u001b[36m current\u001b[0m\u001b[36m temperature\u001b[0m\u001b[36m.\u001b[0m\u001b[36m Since\u001b[0m\u001b[36m I\u001b[0m\u001b[36m don\u001b[0m\u001b[36m't\u001b[0m\u001b[36m have\u001b[0m\u001b[36m real\u001b[0m\u001b[36m-time\u001b[0m\u001b[36m data\u001b[0m\u001b[36m,\u001b[0m\u001b[36m I\u001b[0m\u001b[36m should\u001b[0m\u001b[36m use\u001b[0m\u001b[36m the\u001b[0m\u001b[36m get\u001b[0m\u001b[36m_current\u001b[0m\u001b[36m_weather\u001b[0m\u001b[36m function\u001b[0m\u001b[36m.\u001b[0m\u001b[36m \n",
      "\n",
      "\u001b[0m\u001b[36mFirst\u001b[0m\u001b[36m,\u001b[0m\u001b[36m the\u001b[0m\u001b[36m function\u001b[0m\u001b[36m requires\u001b[0m\u001b[36m the\u001b[0m\u001b[36m city\u001b[0m\u001b[36m,\u001b[0m\u001b[36m state\u001b[0m\u001b[36m,\u001b[0m\u001b[36m and\u001b[0m\u001b[36m unit\u001b[0m\u001b[36m.\u001b[0m\u001b[36m NYC\u001b[0m\u001b[36m is\u001b[0m\u001b[36m in\u001b[0m\u001b[36m New\u001b[0m\u001b[36m York\u001b[0m\u001b[36m,\u001b[0m\u001b[36m so\u001b[0m\u001b[36m the\u001b[0m\u001b[36m state\u001b[0m\u001b[36m abbreviation\u001b[0m\u001b[36m is\u001b[0m\u001b[36m NY\u001b[0m\u001b[36m.\u001b[0m\u001b[36m The\u001b[0m\u001b[36m user\u001b[0m\u001b[36m didn\u001b[0m\u001b[36m't\u001b[0m\u001b[36m specify\u001b[0m\u001b[36m Celsius\u001b[0m\u001b[36m or\u001b[0m\u001b[36m Fahrenheit\u001b[0m\u001b[36m,\u001b[0m\u001b[36m but\u001b[0m\u001b[36m since\u001b[0m\u001b[36m it\u001b[0m\u001b[36m's\u001b[0m\u001b[36m the\u001b[0m\u001b[36m US\u001b[0m\u001b[36m,\u001b[0m\u001b[36m Fahrenheit\u001b[0m\u001b[36m is\u001b[0m\u001b[36m probably\u001b[0m\u001b[36m more\u001b[0m\u001b[36m common\u001b[0m\u001b[36m there\u001b[0m\u001b[36m.\u001b[0m\u001b[36m \n",
      "\n",
      "\u001b[0m\u001b[36mI\u001b[0m\u001b[36m'll\u001b[0m\u001b[36m call\u001b[0m\u001b[36m the\u001b[0m\u001b[36m function\u001b[0m\u001b[36m with\u001b[0m\u001b[36m city\u001b[0m\u001b[36m:\u001b[0m\u001b[36m \"\u001b[0m\u001b[36mNew\u001b[0m\u001b[36m York\u001b[0m\u001b[36m\",\u001b[0m\u001b[36m state\u001b[0m\u001b[36m:\u001b[0m\u001b[36m \"\u001b[0m\u001b[36mNY\u001b[0m\u001b[36m\",\u001b[0m\u001b[36m and\u001b[0m\u001b[36m unit\u001b[0m\u001b[36m:\u001b[0m\u001b[36m \"\u001b[0m\u001b[36mf\u001b[0m\u001b[36mahrenheit\u001b[0m\u001b[36m\".\u001b[0m\u001b[36m Once\u001b[0m\u001b[36m I\u001b[0m\u001b[36m get\u001b[0m\u001b[36m the\u001b[0m\u001b[36m temperature\u001b[0m\u001b[36m,\u001b[0m\u001b[36m I\u001b[0m\u001b[36m can\u001b[0m\u001b[36m advise\u001b[0m\u001b[36m on\u001b[0m\u001b[36m the\u001b[0m\u001b[36m clothing\u001b[0m\u001b[36m.\u001b[0m\u001b[36m If\u001b[0m\u001b[36m it\u001b[0m\u001b[36m's\u001b[0m\u001b[36m above\u001b[0m\u001b[36m \u001b[0m\u001b[36m7\u001b[0m\u001b[36m0\u001b[0m\u001b[36mÂ°F\u001b[0m\u001b[36m,\u001b[0m\u001b[36m short\u001b[0m\u001b[36m sleeves\u001b[0m\u001b[36m might\u001b[0m\u001b[36m be\u001b[0m\u001b[36m better\u001b[0m\u001b[36m.\u001b[0m\u001b[36m If\u001b[0m\u001b[36m it\u001b[0m\u001b[36m's\u001b[0m\u001b[36m cooler\u001b[0m\u001b[36m,\u001b[0m\u001b[36m long\u001b[0m\u001b[36m sleeves\u001b[0m\u001b[36m.\u001b[0m\u001b[36m But\u001b[0m\u001b[36m I\u001b[0m\u001b[36m need\u001b[0m\u001b[36m the\u001b[0m\u001b[36m actual\u001b[0m\u001b[36m data\u001b[0m\u001b[36m to\u001b[0m\u001b[36m make\u001b[0m\u001b[36m a\u001b[0m\u001b[36m recommendation\u001b[0m\u001b[36m.\n",
      "\u001b[0m\n",
      "\n",
      "\n",
      "\n",
      "ðŸ”§ Model requested tool: get_current_weather\n",
      "   with args: {'city': 'New York', 'state': 'NY', 'unit': 'fahrenheit'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-11 12:15:04,669 - httpx - INFO - HTTP Request: POST http://localhost:51105/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”§ Tool response: {'temperature': 100, 'unit': 'fahrenheit', 'city': 'New York', 'state': 'NY', 'description': 'clear skies with a light breeze'}\n",
      "\n",
      "\n",
      "ðŸ’­ Getting final answer...\n",
      "\n",
      "\u001b[36m\n",
      "\u001b[0m\u001b[36mOkay\u001b[0m\u001b[36m,\u001b[0m\u001b[36m the\u001b[0m\u001b[36m user\u001b[0m\u001b[36m is\u001b[0m\u001b[36m asking\u001b[0m\u001b[36m whether\u001b[0m\u001b[36m to\u001b[0m\u001b[36m wear\u001b[0m\u001b[36m short\u001b[0m\u001b[36m sleeves\u001b[0m\u001b[36m or\u001b[0m\u001b[36m long\u001b[0m\u001b[36m sleeves\u001b[0m\u001b[36m in\u001b[0m\u001b[36m NYC\u001b[0m\u001b[36m today\u001b[0m\u001b[36m.\u001b[0m\u001b[36m I\u001b[0m\u001b[36m need\u001b[0m\u001b[36m to\u001b[0m\u001b[36m check\u001b[0m\u001b[36m the\u001b[0m\u001b[36m weather\u001b[0m\u001b[36m.\u001b[0m\u001b[36m I\u001b[0m\u001b[36m called\u001b[0m\u001b[36m the\u001b[0m\u001b[36m get\u001b[0m\u001b[36m_current\u001b[0m\u001b[36m_weather\u001b[0m\u001b[36m tool\u001b[0m\u001b[36m with\u001b[0m\u001b[36m New\u001b[0m\u001b[36m York\u001b[0m\u001b[36m,\u001b[0m\u001b[36m NY\u001b[0m\u001b[36m,\u001b[0m\u001b[36m and\u001b[0m\u001b[36m f\u001b[0m\u001b[36mahrenheit\u001b[0m\u001b[36m.\u001b[0m\u001b[36m The\u001b[0m\u001b[36m response\u001b[0m\u001b[36m says\u001b[0m\u001b[36m the\u001b[0m\u001b[36m temperature\u001b[0m\u001b[36m is\u001b[0m\u001b[36m \u001b[0m\u001b[36m1\u001b[0m\u001b[36m0\u001b[0m\u001b[36m0\u001b[0m\u001b[36mÂ°F\u001b[0m\u001b[36m with\u001b[0m\u001b[36m clear\u001b[0m\u001b[36m skies\u001b[0m\u001b[36m and\u001b[0m\u001b[36m a\u001b[0m\u001b[36m light\u001b[0m\u001b[36m breeze\u001b[0m\u001b[36m.\n",
      "\n",
      "\u001b[0m\u001b[36mHmm\u001b[0m\u001b[36m,\u001b[0m\u001b[36m \u001b[0m\u001b[36m1\u001b[0m\u001b[36m0\u001b[0m\u001b[36m0\u001b[0m\u001b[36mÂ°F\u001b[0m\u001b[36m is\u001b[0m\u001b[36m quite\u001b[0m\u001b[36m hot\u001b[0m\u001b[36m.\u001b[0m\u001b[36m Short\u001b[0m\u001b[36m sleeves\u001b[0m\u001b[36m would\u001b[0m\u001b[36m be\u001b[0m\u001b[36m better\u001b[0m\u001b[36m to\u001b[0m\u001b[36m stay\u001b[0m\u001b[36m cool\u001b[0m\u001b[36m.\u001b[0m\u001b[36m But\u001b[0m\u001b[36m maybe\u001b[0m\u001b[36m suggest\u001b[0m\u001b[36m some\u001b[0m\u001b[36m tips\u001b[0m\u001b[36m to\u001b[0m\u001b[36m avoid\u001b[0m\u001b[36m the\u001b[0m\u001b[36m sun\u001b[0m\u001b[36m,\u001b[0m\u001b[36m like\u001b[0m\u001b[36m using\u001b[0m\u001b[36m sunscreen\u001b[0m\u001b[36m or\u001b[0m\u001b[36m a\u001b[0m\u001b[36m hat\u001b[0m\u001b[36m.\u001b[0m\u001b[36m Also\u001b[0m\u001b[36m,\u001b[0m\u001b[36m mention\u001b[0m\u001b[36m that\u001b[0m\u001b[36m if\u001b[0m\u001b[36m they\u001b[0m\u001b[36m're\u001b[0m\u001b[36m near\u001b[0m\u001b[36m water\u001b[0m\u001b[36m,\u001b[0m\u001b[36m a\u001b[0m\u001b[36m light\u001b[0m\u001b[36m long\u001b[0m\u001b[36m sleeve\u001b[0m\u001b[36m might\u001b[0m\u001b[36m help\u001b[0m\u001b[36m with\u001b[0m\u001b[36m the\u001b[0m\u001b[36m breeze\u001b[0m\u001b[36m.\u001b[0m\u001b[36m But\u001b[0m\u001b[36m overall\u001b[0m\u001b[36m,\u001b[0m\u001b[36m short\u001b[0m\u001b[36m sleeves\u001b[0m\u001b[36m are\u001b[0m\u001b[36m the\u001b[0m\u001b[36m way\u001b[0m\u001b[36m to\u001b[0m\u001b[36m go\u001b[0m\u001b[36m today\u001b[0m\u001b[36m.\n",
      "\u001b[0m\n",
      "\n",
      "Given that it's 100Â°F in New York City today with clear skies, **short sleeves** are definitely the way to go! Hereâ€™s a quick breakdown to help you stay comfortable:\n",
      "\n",
      "- **Short sleeves** will keep you cool in the heat.  \n",
      "- Pair them with **lightweight, breathable fabric** (like cotton or linen) for maximum comfort.  \n",
      "- Donâ€™t forget **sunscreen** and a hat, as UV rays can be strong even on clear days.  \n",
      "\n",
      "If youâ€™re near the coast or in a breezy area, a **lightweight long sleeve** (like a linen shirt) could help block the sun without adding too much warmth. But for most of the city, stick to short sleeves and stay hydrated! ðŸŒž\n",
      "\n",
      "ðŸŽ‰ **Done!**\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": (\n",
    "            \"You are a helpful assistant. When you receive tool outputs, \"\n",
    "            \"always provide a complete and helpful final answer to the user's question. \"\n",
    "            \"Be conversational and provide practical advice.\"\n",
    "        ),\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"should i wear short sleeves or long sleeves in nyc today?\",\n",
    "    },\n",
    "]\n",
    "\n",
    "print(\"\\nðŸ“¡ **Streaming** â€” watch the tokens, tool call, and final answer arrive\\n\")\n",
    "\n",
    "# First API call with tools\n",
    "response_stream = openai_client.chat.completions.create(\n",
    "    model=\"model\", \n",
    "    messages=messages,\n",
    "    tools=tools,\n",
    "    tool_choice=\"auto\",  # Let the model decide when to use tools\n",
    "    stream=True,\n",
    ")\n",
    "\n",
    "# Track tool calls as they stream in\n",
    "tool_calls = []\n",
    "current_tool_call = None\n",
    "accumulated_content = \"\"\n",
    "\n",
    "for chunk in response_stream:\n",
    "    if not chunk.choices:\n",
    "        continue\n",
    "    \n",
    "    choice = chunk.choices[0]\n",
    "    delta = choice.delta\n",
    "    \n",
    "    # Handle reasoning content (if your model supports it)\n",
    "    if hasattr(delta, \"reasoning_content\") and delta.reasoning_content:\n",
    "        print(\"\\033[36m\" + delta.reasoning_content + \"\\033[0m\", end=\"\", flush=True)\n",
    "    \n",
    "    # Handle regular content\n",
    "    if delta.content:\n",
    "        accumulated_content += delta.content\n",
    "        print(delta.content, end=\"\", flush=True)\n",
    "    \n",
    "    # Handle tool calls\n",
    "    if delta.tool_calls:\n",
    "        for tc_delta in delta.tool_calls:\n",
    "            # Start a new tool call\n",
    "            if tc_delta.index == 0 and not current_tool_call:\n",
    "                current_tool_call = {\n",
    "                    \"id\": tc_delta.id or f\"call_{uuid.uuid4().hex[:8]}\",\n",
    "                    \"type\": \"function\",\n",
    "                    \"function\": {\n",
    "                        \"name\": tc_delta.function.name,\n",
    "                        \"arguments\": \"\"\n",
    "                    }\n",
    "                }\n",
    "            \n",
    "            # Accumulate arguments\n",
    "            if tc_delta.function.arguments:\n",
    "                current_tool_call[\"function\"][\"arguments\"] += tc_delta.function.arguments\n",
    "\n",
    "# Execute tool calls if any\n",
    "if choice.finish_reason == \"tool_calls\" and current_tool_call:\n",
    "    print(f\"\\n\\nðŸ”§ Model requested tool: {current_tool_call['function']['name']}\")\n",
    "    \n",
    "    # Parse arguments and execute\n",
    "    args = json.loads(current_tool_call[\"function\"][\"arguments\"])\n",
    "    print(f\"   with args: {args}\")\n",
    "    \n",
    "    tool_name = current_tool_call[\"function\"][\"name\"]\n",
    "    tool_result = local_tool_registry[tool_name](**args)\n",
    "    print(f\"ðŸ”§ Tool response: {tool_result}\")\n",
    "    \n",
    "    # Add the assistant's tool call message\n",
    "    messages.append({\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": accumulated_content if accumulated_content else None,\n",
    "        \"tool_calls\": [current_tool_call]\n",
    "    })\n",
    "    \n",
    "    # Add the tool response\n",
    "    messages.append({\n",
    "        \"role\": \"tool\",\n",
    "        \"tool_call_id\": current_tool_call[\"id\"],\n",
    "        \"name\": tool_name,\n",
    "        \"content\": json.dumps(tool_result)\n",
    "    })\n",
    "    \n",
    "    # Make a follow-up call to get the final answer\n",
    "    print(\"\\n\\nðŸ’­ Getting final answer...\\n\")\n",
    "    \n",
    "    follow_stream = openai_client.chat.completions.create(\n",
    "        model=\"model\",\n",
    "        messages=messages,\n",
    "        stream=True,\n",
    "        # No tools this time - we want a final answer\n",
    "    )\n",
    "    \n",
    "    # Stream the final response\n",
    "    for follow_chunk in follow_stream:\n",
    "        if not follow_chunk.choices:\n",
    "            continue\n",
    "        \n",
    "        follow_choice = follow_chunk.choices[0]\n",
    "        follow_delta = follow_choice.delta\n",
    "        \n",
    "        # Handle reasoning content\n",
    "        if hasattr(follow_delta, \"reasoning_content\") and follow_delta.reasoning_content:\n",
    "            print(\"\\033[36m\" + follow_delta.reasoning_content + \"\\033[0m\", end=\"\", flush=True)\n",
    "        \n",
    "        # Handle final answer content\n",
    "        if follow_delta.content:\n",
    "            print(follow_delta.content, end=\"\", flush=True)\n",
    "    \n",
    "    print(\"\\n\\nðŸŽ‰ **Done!**\")\n",
    "\n",
    "# If no tool was called, we're done\n",
    "elif choice.finish_reason == \"stop\":\n",
    "    print(\"\\n\\nðŸŽ‰ **Done!** (No tool calls needed)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f18a82-683b-4860-8b3e-a7e6b27988b8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
